{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:02.934131-04:00",
     "start_time": "2017-06-10T09:51:02.925574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = \"Hello SDw:sd, and the in on not asda d. adf! 989.09 6:09 adadsa-adas.\"\n",
    "txt2 = \"Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.\"\n",
    "txt3 = \"Hello SDw:sd, asda d. adf! 989.adas.Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.\"\n",
    "txt4 = \"Hello SDw:sd, asda d. a\"\n",
    "txt5 = \"Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.Hello SDw:sd, asda d. abs iojknas sdfdsd\"\n",
    "txt10 = \"Hello SDw:sd, asda d. adf! 9df frfw3 as 1 -adas.Hello SDw:sd, asda d. adf! 989.09 6:09 adadsa-adas.Hello SDw:sd, asda d. abs\"\n",
    "\n",
    "docs = {\"d1\": txt, \"d2\": txt2, \"d3\": txt3, \"d4\": txt4, \"d5\": txt5}\n",
    "docs2 = {\"d6\": txt, \"d7\": txt2, \"d8\": txt3, \"d9\": txt4, \"d10\": txt10}\n",
    "\n",
    "# TODO: loadTermMap() and docMap\n",
    "termMap = []\n",
    "docMap = []\n",
    "catalog = defaultdict(int)\n",
    "\n",
    "merge_count = 0\n",
    "doc_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:03.220490-04:00",
     "start_time": "2017-06-10T09:51:03.216084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:03.578344-04:00",
     "start_time": "2017-06-10T09:51:03.568491Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return list(filter(lambda x: x != \"\", re.compile(r'[\\s,!-\\.]').split(text.lower())))\n",
    "\n",
    "def stemmer(term):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "def stopWordsRemoval(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenizer(text)\n",
    "    stokens = map(stemmer, tokens)\n",
    "    return stopWordsRemoval(stokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:03.912709-04:00",
     "start_time": "2017-06-10T09:51:03.905660Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSetItemId(term, someMap):\n",
    "    if term not in someMap:\n",
    "        i = len(someMap)\n",
    "        someMap.append(term)\n",
    "    else:\n",
    "        i = someMap.index(term)\n",
    "    return i\n",
    "\n",
    "def getItemName(id, someMap):\n",
    "    return someMap[id]\n",
    "\n",
    "assert(getSetItemId(\"hello\", [])) == 0\n",
    "assert(getSetItemId(\"d1\", [])) == 0\n",
    "\n",
    "assert(getItemName(0, [\"hello\"])) == \"hello\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:04.282898-04:00",
     "start_time": "2017-06-10T09:51:04.267269Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTermList(docs):\n",
    "    itermCount = defaultdict(dict)\n",
    "    df = defaultdict(int)\n",
    "    ttf = defaultdict(int)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_tokens = preprocess(docs[doc])\n",
    "        for token in enumerate(doc_tokens):\n",
    "            pos = token[0]\n",
    "            tokenId = getItemId(token[1], termMap)\n",
    "            docId = getItemId(doc, docMap)\n",
    "\n",
    "            if tokenId not in itermCount.keys():\n",
    "                itermCount[tokenId] = {}\n",
    "            if docId not in itermCount[tokenId].keys():\n",
    "                itermCount[tokenId][docId] = {}\n",
    "                itermCount[tokenId][docId]['tf'] = 0\n",
    "                itermCount[tokenId][docId]['pos'] = []\n",
    "            itermCount[tokenId][docId]['tf'] += 1\n",
    "            itermCount[tokenId][docId]['pos'].append(pos)\n",
    "    for tokenId, docs in itermCount.items():\n",
    "        df[tokenId] = len(docs.keys())\n",
    "    for tokenId, docs in itermCount.items():\n",
    "        ttf[tokenId] = sum(map(lambda d: d['tf'], docs.values()))\n",
    "    return (itermCount, df, ttf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:04.635746-04:00",
     "start_time": "2017-06-10T09:51:04.623010Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeTermFile(filename, itermCount, df, ttf):\n",
    "    catalog = defaultdict(int)\n",
    "    with open(filename, \"w+\") as out:\n",
    "        for termId, docs in itermCount.items():\n",
    "            catalog[termId] = out.tell()\n",
    "            outStr = str(termId) + \":\" + str(df[termId]) + \":\" + str(ttf[termId]) + \":\" \n",
    "            tempStr = []\n",
    "            for docId, values in docs.items():\n",
    "                tempStr.append(str(docId) + \",\" + str(values['tf']) + \",\" + \"{}\".format(\".\".join(map(lambda x: str(x), values['pos']))))\n",
    "            outStr += \"|\".join(tempStr)\n",
    "            print(outStr)\n",
    "            out.write(outStr+\"\\n\")\n",
    "    return catalog\n",
    "\n",
    "def writeCatalog(filename, catalog):\n",
    "    with open(filename, \"w+\") as out:\n",
    "        for termId, offset in catalog.items():\n",
    "            out.write(termId + \",\" + offset + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:04.968342-04:00",
     "start_time": "2017-06-10T09:51:04.953398Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTermFromFile(filename, termIdRef, catalog):\n",
    "    ntc = {}\n",
    "    f = open(filename)\n",
    "    f.seek(catalog[termIdRef])\n",
    "    line = f.readline().strip()\n",
    "    (termId, df, ttf, docs) = line.strip().split(\":\")\n",
    "    mdocs = docs.split(\"|\")\n",
    "    ntc= {}\n",
    "    for p in mdocs:\n",
    "        docId, tf, els = p.split(\",\")\n",
    "        ntc[docId] = {}\n",
    "        ntc[docId]['tf'] = int(tf) \n",
    "        ntc[docId]['pos'] = list(map(lambda x: int(x), els.split(\".\"))) \n",
    "    return (ntc, int(df), int(ttf))\n",
    "\n",
    "def readCatalog(catalogFile):\n",
    "    catalog = {}\n",
    "    # Read from file\n",
    "    with open(catalogFile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            termId, offset = line.split(\",\")\n",
    "            catalog[termId] = offset\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:05.262081-04:00",
     "start_time": "2017-06-10T09:51:05.239120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(itermCount1, df1, ttf1) = createTermList(docs)\n",
    "(itermCount2, df2, ttf2) = createTermList(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:05.506336-04:00",
     "start_time": "2017-06-10T09:51:05.499511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:5:9:0,3,0.9.18|1,1,0|2,2,0.6|3,1,0|4,2,0.9\n",
      "1:5:9:0,3,1.10.19|1,1,1|2,2,1.7|3,1,1|4,2,1.10\n",
      "2:5:9:0,3,2.11.20|1,1,2|2,2,2.8|3,1,2|4,2,2.11\n",
      "3:4:7:0,2,3.12|1,1,3|2,2,3.9|4,2,3.12\n",
      "4:4:7:0,2,4.13|1,1,4|2,2,4.10|4,2,4.13\n",
      "5:4:6:0,2,5.14|1,1,5|2,1,11|4,2,5.14\n",
      "6:4:6:0,2,6.15|1,1,6|2,1,12|4,2,6.15\n",
      "7:4:6:0,2,7.16|1,1,7|2,1,13|4,2,7.16\n",
      "8:4:7:0,2,8.17|1,1,8|2,2,5.14|4,2,8.17\n",
      "9:1:1:0,1,21\n",
      "10:1:1:0,1,22\n",
      "11:1:1:0,1,23\n",
      "0:5:9:8,3,0.8.17|9,2,0.6|5,2,0.9|6,1,0|7,1,0\n",
      "1:5:9:8,3,1.9.18|9,2,1.7|5,2,1.10|6,1,1|7,1,1\n",
      "2:5:9:8,3,2.10.19|9,2,2.8|5,2,2.11|6,1,2|7,1,2\n",
      "3:4:7:8,2,3.11|9,2,3.9|5,2,3.12|7,1,3\n",
      "4:4:6:8,1,12|9,2,4.10|5,2,4.13|7,1,4\n",
      "5:4:5:8,1,13|9,1,11|5,2,5.14|7,1,5\n",
      "6:4:5:8,1,14|9,1,12|5,2,6.15|7,1,6\n",
      "7:4:5:8,1,15|9,1,13|5,2,7.16|7,1,7\n",
      "8:4:7:8,2,7.16|9,2,5.14|5,2,8.17|7,1,8\n",
      "9:1:1:8,1,20\n",
      "12:1:1:8,1,4\n",
      "13:1:1:8,1,5\n",
      "14:1:1:8,1,6\n",
      "test1.index test2.index\n"
     ]
    }
   ],
   "source": [
    "doc_count += 1\n",
    "file_name1 = \"test{}.index\".format(doc_count)\n",
    "doc_count += 1\n",
    "file_name2 = \"test{}.index\".format(doc_count)\n",
    "catalog1 = writeTermFile(file_name1, itermCount1, df1, ttf1)\n",
    "catalog2 = writeTermFile(file_name2, itermCount2, df2, ttf2)\n",
    "print(file_name1, file_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:05.937588-04:00",
     "start_time": "2017-06-10T09:51:05.929520Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:10:18:0,3,0.9.18|8,3,0.8.17|9,2,0.6|2,2,0.6|5,2,0.9|4,2,0.9|3,1,0|7,1,0|6,1,0|1,1,0\n",
      "1:10:18:0,3,1.10.19|8,3,1.9.18|9,2,1.7|2,2,1.7|5,2,1.10|4,2,1.10|3,1,1|7,1,1|6,1,1|1,1,1\n",
      "2:10:18:0,3,2.11.20|8,3,2.10.19|9,2,2.8|2,2,2.8|5,2,2.11|4,2,2.11|3,1,2|7,1,2|6,1,2|1,1,2\n",
      "3:8:14:9,2,3.9|2,2,3.9|0,2,3.12|8,2,3.11|5,2,3.12|4,2,3.12|7,1,3|1,1,3\n",
      "4:8:13:9,2,4.10|2,2,4.10|0,2,4.13|5,2,4.13|4,2,4.13|8,1,12|7,1,4|1,1,4\n",
      "5:8:11:0,2,5.14|5,2,5.14|4,2,5.14|9,1,11|2,1,11|8,1,13|7,1,5|1,1,5\n",
      "6:8:11:0,2,6.15|5,2,6.15|4,2,6.15|9,1,12|2,1,12|8,1,14|7,1,6|1,1,6\n",
      "7:8:11:0,2,7.16|5,2,7.16|4,2,7.16|9,1,13|2,1,13|8,1,15|7,1,7|1,1,7\n",
      "8:8:14:9,2,5.14|2,2,5.14|0,2,8.17|8,2,7.16|5,2,8.17|4,2,8.17|7,1,8|1,1,8\n",
      "9:2:2:8,1,20|0,1,21\n",
      "10:1:1:0,1,22\n",
      "11:1:1:0,1,23\n",
      "12:1:1:8,1,4\n",
      "13:1:1:8,1,5\n",
      "14:1:1:8,1,6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merge1.index',\n",
       " defaultdict(int,\n",
       "             {0: 0,\n",
       "              1: 86,\n",
       "              2: 175,\n",
       "              3: 265,\n",
       "              4: 336,\n",
       "              5: 407,\n",
       "              6: 474,\n",
       "              7: 541,\n",
       "              8: 608,\n",
       "              9: 681,\n",
       "              10: 701,\n",
       "              11: 715,\n",
       "              12: 729,\n",
       "              13: 742,\n",
       "              14: 755}))"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge(catalog1, catalog2, file_name1, file_name2, merge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:06.167265-04:00",
     "start_time": "2017-06-10T09:51:06.157860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeDocs(termId, filename, catalog, n_docs, df1, ttf1):\n",
    "    merged_termCount = {}\n",
    "    df_merge = df1\n",
    "    ttf_merge = ttf1\n",
    "#     print(n_docs)\n",
    "    old_docs, df2, ttf2 = readTermFromFile(filename, termId, catalog)\n",
    "#     print(old_docs)\n",
    "    join_docs = dict()\n",
    "    join_docs.update(n_docs)\n",
    "    join_docs.update(old_docs)\n",
    "    newdocs = OrderedDict(sorted(join_docs.values(), key=lambda x: x[\"tf\"], reverse=True))\n",
    "\n",
    "    merged_termCount[termId] = newdocs\n",
    "    df_merge += int(df2)\n",
    "    ttf_merge += int(ttf2)\n",
    "        \n",
    "    return (newdocs, df_merge, ttf_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:06.667034-04:00",
     "start_time": "2017-06-10T09:51:06.649682Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeTempData(itermCount, df, ttf):\n",
    "    filename = \"temp.index\"\n",
    "    with open(filename, \"a\") as out:\n",
    "        for termId, docs in itermCount.items():\n",
    "            if termId in catalog.keys():\n",
    "                res = mergeDocs(termId, filename, catalog, docs, df[termId], ttf[termId])\n",
    "                docs = res[0]\n",
    "                df[termId] = res[1]\n",
    "                ttf[termId] = res[2]\n",
    "            catalog[termId] = out.tell()\n",
    "            outStr = str(termId) + \":\" + str(df[termId]) + \":\" + str(ttf[termId]) + \":\" \n",
    "            tempStr = []\n",
    "            for docId, values in docs.items():\n",
    "                tempStr.append(str(docId) + \",\" + str(values['tf']) + \",\" + \"{}\".format(\".\".join(map(lambda x: str(x), values['pos']))))\n",
    "            outStr += \"|\".join(tempStr)\n",
    "            print(outStr)\n",
    "            out.write(outStr+\"\\n\")\n",
    "    return catalog\n",
    "\n",
    "def writeCatalog(filename, catalog):\n",
    "    with open(filename, \"w+\") as out:\n",
    "        for termId, offset in catalog.items():\n",
    "            out.write(termId + \",\" + offset + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:09.230319-04:00",
     "start_time": "2017-06-10T09:51:09.225081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:5:9:0,3,0.9.18|1,1,0|2,2,0.6|3,1,0|4,2,0.9\n",
      "1:5:9:0,3,1.10.19|1,1,1|2,2,1.7|3,1,1|4,2,1.10\n",
      "2:5:9:0,3,2.11.20|1,1,2|2,2,2.8|3,1,2|4,2,2.11\n",
      "3:4:7:0,2,3.12|1,1,3|2,2,3.9|4,2,3.12\n",
      "4:4:7:0,2,4.13|1,1,4|2,2,4.10|4,2,4.13\n",
      "5:4:6:0,2,5.14|1,1,5|2,1,11|4,2,5.14\n",
      "6:4:6:0,2,6.15|1,1,6|2,1,12|4,2,6.15\n",
      "7:4:6:0,2,7.16|1,1,7|2,1,13|4,2,7.16\n",
      "8:4:7:0,2,8.17|1,1,8|2,2,5.14|4,2,8.17\n",
      "9:1:1:0,1,21\n",
      "10:1:1:0,1,22\n",
      "11:1:1:0,1,23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 407,\n",
       "             1: 452,\n",
       "             2: 499,\n",
       "             3: 546,\n",
       "             4: 584,\n",
       "             5: 623,\n",
       "             6: 660,\n",
       "             7: 697,\n",
       "             8: 734,\n",
       "             9: 773,\n",
       "             10: 786,\n",
       "             11: 800})"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeTempData(itermCount1, df1, ttf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:51:11.444892-04:00",
     "start_time": "2017-06-10T09:51:11.433065Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-9588ee9bfb04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriteTempData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitermCount2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-391-db7f1cf49d37>\u001b[0m in \u001b[0;36mwriteTempData\u001b[0;34m(itermCount, df, ttf)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtempStr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mtempStr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moutStr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempStr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutStr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "writeTempData(itermCount2, df2, ttf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T20:16:44.141849-04:00",
     "start_time": "2017-06-08T20:16:44.138213Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class DocTerm:\n",
    "#     def __init__(self, tf, pos):\n",
    "#         self.tf = tf\n",
    "#         self.pos = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T20:17:27.947398-04:00",
     "start_time": "2017-06-08T20:17:27.943932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# termCount = {}\n",
    "# for doc in docs:\n",
    "#     termCount[doc] = defaultdict(int)\n",
    "# termCount\n",
    "\n",
    "# docs = docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T21:24:26.297517-04:00",
     "start_time": "2017-05-31T21:24:26.294844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BasicLoadInvList(term):\n",
    "# \toff, length from catalog\n",
    "# \tmove invfilecounter to offset\n",
    "# \tread length of the term\n",
    "# \tparse the invertlst into a local DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T21:24:41.704705-04:00",
     "start_time": "2017-05-31T21:24:41.701481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load both catalogs in the memory\n",
    "# Pick first term in invfile 1\n",
    "# \tinvList_term1 = load invfile for offset, length of term 1\n",
    "# \tscan catalog2 for term2\n",
    "# \tif yes:\n",
    "# \t\tget invList_term2 = load from file2\n",
    "# \t\tjoin the two lists\n",
    "# \twrite (inv_combined)\n",
    "# \twrite (catalog_combined)\n",
    "\n",
    "# Sort the docs in decreasing order of tf count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-10T09:42:33.287387-04:00",
     "start_time": "2017-06-10T09:42:33.257688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(catalog1, catalog2, file1, file2, merge_count):\n",
    "    merged_termCount = {}\n",
    "    df_merge = {}\n",
    "    ttf_merge = {}\n",
    "\n",
    "    common_terms = set(catalog1.keys()).intersection(set(catalog2.keys()))\n",
    "    # print(list(common_terms))\n",
    "    unique_terms = set(catalog1.keys()).union(set(catalog2.keys())) - common_terms\n",
    "    # print(unique_terms)\n",
    "\n",
    "    for termId in common_terms:\n",
    "        temp_docs1, df1, ttf1 = readTermFromFile(file1, termId, catalog1)\n",
    "        temp_docs2, df2, ttf2 = readTermFromFile(file2, termId, catalog2)\n",
    "        join_docs = dict()\n",
    "        join_docs.update(temp_docs1)\n",
    "        join_docs.update(temp_docs2)\n",
    "        newdocs = OrderedDict(sorted(join_docs.items(), key=lambda x: x[1][\"tf\"], reverse=True))\n",
    "\n",
    "        merged_termCount[termId] = newdocs\n",
    "        df_merge[termId] = int(df1) + int(df2)\n",
    "        ttf_merge[termId] = int(ttf1) + int(ttf2)\n",
    "\n",
    "    for termId in set(catalog1.keys()).difference(set(catalog2.keys())):\n",
    "        temp_docs, df, ttf = readTermFromFile(file1, termId, catalog1)\n",
    "        merged_termCount[termId] = temp_docs\n",
    "        df_merge[termId] = df\n",
    "        ttf_merge[termId] = ttf\n",
    "\n",
    "    for termId in set(catalog2.keys()).difference(set(catalog1.keys())):\n",
    "        temp_docs, df, ttf = readTermFromFile(file2, termId, catalog2)\n",
    "        merged_termCount[termId] = temp_docs\n",
    "        df_merge[termId] = df\n",
    "        ttf_merge[termId] = ttf\n",
    "        \n",
    "    merge_count += 1\n",
    "    merge_index_name = \"merge\"+str(merge_count)+\".index\"\n",
    "    merge_catalog = writeTermFile(merge_index_name, merged_termCount, df_merge, ttf_merge)\n",
    "    return (merge_index_name, merge_catalog)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
